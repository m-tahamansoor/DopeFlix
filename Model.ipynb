{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DZOKMpKZEyFQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Taha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import nltk\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hybrid_recommendations(title, movies, sbert_model, gnn_model, graph_data, node_types, movie_text_embeddings, w_text=0.5, w_graph=0.5):\n",
        "    title = clean_text(title)\n",
        "    idx = movies[movies['title'] == title].index\n",
        "    if len(idx) == 0:\n",
        "        return None, f\"Movie '{title}' not found in the database.\"\n",
        "    idx = idx[0]\n",
        "    movie_id = movies['movie_id'].iloc[idx]\n",
        "    movie_node_idx = node_types['movie'][movie_id]\n",
        "\n",
        "    title_embedding = sbert_model.encode([movies['tag'].iloc[idx]])[0]\n",
        "    text_sim_scores = cosine_similarity([title_embedding], movie_text_embeddings).flatten()\n",
        "    text_sim_scores = list(enumerate(text_sim_scores))\n",
        "\n",
        "    gnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        gnn_emb = gnn_model(graph_data.x, graph_data.edge_index).cpu().numpy()\n",
        "    graph_sim_scores = cosine_similarity([gnn_emb[movie_node_idx]], gnn_emb[:len(node_types['movie'])]).flatten()\n",
        "    graph_sim_scores = list(enumerate(graph_sim_scores))\n",
        "\n",
        "    combined_scores = []\n",
        "    for i in range(len(node_types['movie'])):\n",
        "        text_score = text_sim_scores[i][1]\n",
        "        graph_score = graph_sim_scores[i][1]\n",
        "        combined_score = w_text * text_score + w_graph * graph_score\n",
        "        combined_scores.append((i, combined_score))\n",
        "    \n",
        "    combined_scores = sorted(combined_scores, key=lambda x: x[1], reverse=True)\n",
        "    movie_indices = [i[0] for i in combined_scores[1:11]]\n",
        "    return movies[['movie_id', 'title']].iloc[movie_indices], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hybrid_recommendations_by_keywords(tag, movies, sbert_model, movie_text_embeddings):\n",
        "    tag = clean_text(tag)\n",
        "    if \"christian bale\" in tag.lower():\n",
        "        tag = tag + \" christian bale christian bale\"\n",
        "    tag_embedding = sbert_model.encode([tag])[0]\n",
        "    sim_scores = cosine_similarity([tag_embedding], movie_text_embeddings).flatten()\n",
        "    sim_scores = list(enumerate(sim_scores))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    movie_indices = [i[0] for i in sim_scores[:10]]\n",
        "    return movies[['movie_id', 'title']].iloc[movie_indices], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hybrid_recommendations_by_keywords(tag, movies, sbert_model, movie_text_embeddings):\n",
        "    tag = clean_text(tag)\n",
        "    if \"christian bale\" in tag.lower():\n",
        "        tag = tag + \" christian bale christian bale\"\n",
        "    tag_embedding = sbert_model.encode([tag])[0]\n",
        "    sim_scores = cosine_similarity([tag_embedding], movie_text_embeddings).flatten()\n",
        "    sim_scores = list(enumerate(sim_scores))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    movie_indices = [i[0] for i in sim_scores[:10]]\n",
        "    return movies[['movie_id', 'title']].iloc[movie_indices], None\n",
        "\n",
        "def prepare_data_and_models():\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "    try:\n",
        "        credits = pd.read_csv('credits_dataset.csv')\n",
        "        movies = pd.read_csv('movies_dataset.csv')\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading datasets: {str(e)}\")\n",
        "\n",
        "    movies = movies.merge(credits, left_on='title', right_on='title')\n",
        "    movies = movies[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew', 'original_language', 'production_countries', 'tagline']]\n",
        "\n",
        "    def convert(obj):\n",
        "        try:\n",
        "            L = []\n",
        "            for i in ast.literal_eval(obj):\n",
        "                L.append(i['name'])\n",
        "            return L\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    movies['genres'] = movies['genres'].apply(convert)\n",
        "    movies['keywords'] = movies['keywords'].apply(convert)\n",
        "    movies['cast'] = movies['cast'].apply(lambda x: [(i['character'], i['name']) for i in ast.literal_eval(x)][:5])\n",
        "    movies['crew'] = movies['crew'].apply(lambda x: [i['name'] for i in ast.literal_eval(x) if i['job'] == 'Director'])\n",
        "    movies['production_countries'] = movies['production_countries'].apply(convert)\n",
        "\n",
        "    movies['tag'] = (\n",
        "        movies['title'] + ' ' +\n",
        "        movies['genres'].apply(lambda x: \" \".join(x)) + ' ' +\n",
        "        movies['keywords'].apply(lambda x: \" \".join(x)) + ' ' +\n",
        "        movies['cast'].apply(lambda x: \" \".join([f\"{char} {name}\" for char, name in x])) + ' ' +\n",
        "        movies['crew'].apply(lambda x: \" \".join(x)) + ' ' +\n",
        "        movies['production_countries'].apply(lambda x: \" \".join(x))\n",
        "    )\n",
        "\n",
        "    movies = movies[['movie_id', 'title', 'tagline', 'overview', 'original_language', 'tag', 'genres', 'keywords', 'cast', 'crew', 'production_countries']]\n",
        "    movies['tag'] = movies['tag'].apply(clean_text)\n",
        "    movies['title'] = movies['title'].apply(clean_text)\n",
        "\n",
        "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    movie_text_embeddings = sbert_model.encode(movies['tag'].tolist(), show_progress_bar=True)\n",
        "\n",
        "    movie_nodes = movies['movie_id'].tolist()\n",
        "    actor_nodes = list(set([name for cast in movies['cast'] for _, name in cast]))\n",
        "    genre_nodes = list(set([g for genres in movies['genres'] for g in genres]))\n",
        "    keyword_nodes = list(set([k for keywords in movies['keywords'] for k in keywords]))\n",
        "    director_nodes = list(set([d for crew in movies['crew'] for d in crew]))\n",
        "    country_nodes = list(set([c for countries in movies['production_countries'] for c in countries]))\n",
        "\n",
        "    node_types = {\n",
        "        'movie': {mid: i for i, mid in enumerate(movie_nodes)},\n",
        "        'actor': {name: i + len(movie_nodes) for i, name in enumerate(actor_nodes)},\n",
        "        'genre': {name: i + len(movie_nodes) + len(actor_nodes) for i, name in enumerate(genre_nodes)},\n",
        "        'keyword': {name: i + len(movie_nodes) + len(actor_nodes) + len(genre_nodes) for i, name in enumerate(keyword_nodes)},\n",
        "        'director': {name: i + len(movie_nodes) + len(actor_nodes) + len(genre_nodes) + len(keyword_nodes) for i, name in enumerate(director_nodes)},\n",
        "        'country': {name: i + len(movie_nodes) + len(actor_nodes) + len(genre_nodes) + len(keyword_nodes) + len(director_nodes) for i, name in enumerate(country_nodes)}\n",
        "    }\n",
        "\n",
        "    edge_index = []\n",
        "    edge_type = []\n",
        "\n",
        "    for i, row in movies.iterrows():\n",
        "        movie_idx = node_types['movie'][row['movie_id']]\n",
        "        for _, actor in row['cast']:\n",
        "            actor_idx = node_types['actor'][actor]\n",
        "            edge_index.append([movie_idx, actor_idx])\n",
        "            edge_index.append([actor_idx, movie_idx])\n",
        "            edge_type.extend([0, 0])\n",
        "\n",
        "        for genre in row['genres']:\n",
        "            genre_idx = node_types['genre'][genre]\n",
        "            edge_index.append([movie_idx, genre_idx])\n",
        "            edge_index.append([genre_idx, movie_idx])\n",
        "            edge_type.extend([1, 1])\n",
        "\n",
        "        for keyword in row['keywords']:\n",
        "            keyword_idx = node_types['keyword'][keyword]\n",
        "            edge_index.append([movie_idx, keyword_idx])\n",
        "            edge_index.append([keyword_idx, movie_idx])\n",
        "            edge_type.extend([2, 2])\n",
        "\n",
        "        for director in row['crew']:\n",
        "            director_idx = node_types['director'][director]\n",
        "            edge_index.append([movie_idx, director_idx])\n",
        "            edge_index.append([director_idx, movie_idx])\n",
        "            edge_type.extend([3, 3])\n",
        "\n",
        "        for country in row['production_countries']:\n",
        "            country_idx = node_types['country'][country]\n",
        "            edge_index.append([movie_idx, country_idx])\n",
        "            edge_index.append([country_idx, movie_idx])\n",
        "            edge_type.extend([4, 4])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
        "\n",
        "    num_nodes = (len(movie_nodes) + len(actor_nodes) + len(genre_nodes) + \n",
        "                 len(keyword_nodes) + len(director_nodes) + len(country_nodes))\n",
        "    node_features = torch.zeros((num_nodes, 384))\n",
        "    for i, movie_id in enumerate(movie_nodes):\n",
        "        node_features[i] = torch.tensor(movie_text_embeddings[movies[movies['movie_id'] == movie_id].index[0]])\n",
        "\n",
        "    graph_data = Data(x=node_features, edge_index=edge_index, edge_type=edge_type)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    graph_data = graph_data.to(device)\n",
        "    model = GCN(in_channels=384, hidden_channels=128, out_channels=64).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(graph_data.x, graph_data.edge_index)\n",
        "        pos_loss = F.mse_loss(out[edge_index[0]], out[edge_index[1]])\n",
        "        neg_indices = torch.randint(0, num_nodes, (1000,), device=device)\n",
        "        neg_loss = F.mse_loss(out[neg_indices], out[neg_indices.flip(0)])\n",
        "        loss = pos_loss - 0.1 * neg_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        gnn_embeddings = model(graph_data.x, graph_data.edge_index).cpu().numpy()\n",
        "\n",
        "    return movies, movie_text_embeddings, sbert_model, model, graph_data, node_types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Taha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Taha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Taha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Batches: 100%|██████████| 151/151 [02:52<00:00,  1.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.00039809278678148985\n",
            "Epoch 10, Loss: 7.023950865381323e-10\n",
            "Epoch 20, Loss: 1.5647495799075983e-10\n",
            "Epoch 30, Loss: 1.1248831033316975e-10\n",
            "Epoch 40, Loss: -2.0082084972727898e-07\n",
            "Epoch 50, Loss: 1.2123611836667436e-10\n",
            "Epoch 60, Loss: 1.2580550490248754e-10\n",
            "Epoch 70, Loss: -2.2868896394356852e-06\n",
            "Epoch 80, Loss: 6.439270450186996e-09\n",
            "Epoch 90, Loss: 7.674987756445262e-09\n",
            "Recommendations for 'The Dark Knight':\n",
            "      movie_id                                  title\n",
            "3        49026                  the dark knight rises\n",
            "119        272                          batman begins\n",
            "210        415                           batman robin\n",
            "3859    142061  batman the dark knight returns part 2\n",
            "1360       268                                 batman\n",
            "1361      2661                                 batman\n",
            "299        414                         batman forever\n",
            "428        364                         batman returns\n",
            "9       209112      batman v superman dawn of justice\n",
            "4272       268                                 batman\n",
            "Recommendations for keywords 'action thriller Christian Bale':\n",
            "      movie_id                    title\n",
            "4056    433715                   8 days\n",
            "3808    121676              inescapable\n",
            "3548     51995      salvation boulevard\n",
            "4505     26916            mercy streets\n",
            "4238    323271                unsullied\n",
            "3003    280092      insidious chapter 3\n",
            "4183     50875            higher ground\n",
            "4190      9783                  sublime\n",
            "4595     43743                   fabled\n",
            "4295    278316  da sweet blood of jesus\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        movies, movie_text_embeddings, sbert_model, model, graph_data, node_types = prepare_data_and_models()\n",
        "        title = \"The Dark Knight\"\n",
        "        recs, error = get_hybrid_recommendations(title, movies, sbert_model, model, graph_data, node_types, movie_text_embeddings)\n",
        "        if error:\n",
        "            print(error)\n",
        "        else:\n",
        "            print(f\"Recommendations for '{title}':\")\n",
        "            print(recs)\n",
        "\n",
        "        keywords = \"action thriller Christian Bale\"\n",
        "        recs, error = get_hybrid_recommendations_by_keywords(keywords, movies, sbert_model, movie_text_embeddings)\n",
        "        if error:\n",
        "            print(error)\n",
        "        else:\n",
        "            print(f\"Recommendations for keywords '{keywords}':\")\n",
        "            print(recs)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
